# -*- coding: utf-8 -*-
"""EXploratoryDataAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VTJr9vX8v30TlLhuR94SaRkgo4BP5PHK
"""

import pandas as pd
import numpy as np

path="https://raw.githubusercontent.com/nunnarilabs/ml/master/Automobile_Data/automobile_data.csv"
data=pd.read_csv(path)
data.head()

#replace ? to NaN
data.replace("?",np.nan,inplace=True)
data

#to sum null values
a=data.isnull().sum()
a

#to check how many rows and columns
data.shape

#to list the columns in the dataset
data.columns.tolist()

data.describe()

data.info()

data.min()

data.max()

data.std()

data.corr()

#Drop duplicates
data.drop_duplicates(inplace=True)

data.shape

#mean for normalised-losses
avg=data["normalized-losses"].astype("float").mean(axis=0)
avg

#Replace missing values with avg of normalized-losses
data["normalized-losses"].replace(np.nan,avg,inplace=True)

#mean for bore
avg1=data["bore"].astype("float").mean(axis=0)
avg1

#Replace missing values with avg of bore
data["bore"].replace(np.nan,avg1,inplace=True)

data["stroke"].replace(np.nan,data["stroke"].astype('float').mean(axis=0),inplace=True)
data["horsepower"].replace(np.nan,data["horsepower"].astype('float').mean(axis=0),inplace=True)
data["peak-rpm"].replace(np.nan,data["peak-rpm"].astype('float').mean(axis=0),inplace=True)

#check the frequent value
data["num-of-doors"].value_counts()

#Replace the missing value with frequent values
data["num-of-doors"].replace(np.nan,'four',inplace=True)

#drop the whole row in price column which has NaN values
data.dropna(subset=["price"],axis=0,inplace=True)

data.shape

#reset index because we dropped two rows
data.reset_index(drop=True,inplace=True)

data.isnull().sum()

data.dtypes

data[["bore","stroke"]]=data[["bore","stroke"]].astype("float")
data[["normalized-losses"]]=data[["normalized-losses"]].astype("int")
data[["price"]]=data[["price"]].astype("float")
data[["peak-rpm"]]=data[["peak-rpm"]].astype("float")
data[["horsepower"]]=data[["horsepower"]].astype("float")

data.dtypes

#IDENTIFYING OUTLIERS

from scipy import stats
hp=np.abs(stats.zscore(data['normalized-losses']))
hp

#set a threshold
threshold=hp.max()

#position of the outlier
index_value=np.where(hp>=threshold)
data.iloc[index_value]['normalized-losses']

#Drop the row that has the outlier
newdata=data.drop(index=data.iloc[index_value]["normalized-losses"].index)
newdata

d=data['drive-wheels'].value_counts().to_frame()
d.rename(columns={'drive-wheels':'value_counts'},inplace=True)
d.index.name='drive-wheels'
d

d1=data['normalized-losses'].value_counts().to_frame()
d1.rename(columns={'normalized-losses':'value_counts1'},inplace=True)
d1.index.name='normalized-losses'
d1

#unique value
data["drive-wheels"].unique()

df=data[['drive-wheels','price']]
df.head()

#groupby resuls
df1=data[['drive-wheels','body-style','price']]
gt=df1.groupby(['drive-wheels','body-style'],as_index=False).mean()
gt

gp=gt.pivot(index='drive-wheels',columns='body-style')
gp

a=data.corr()
a

import seaborn as sns
sns.heatmap(data[["normalized-losses"]])

sns.heatmap(a)

data['city-L/100km']=235/data["city-mpg"]

data.head()

from sklearn.preprocessing import StandardScaler
data1=[[1,2],[3,4],[5,6]]
scalar=StandardScaler()
sd=scalar.fit_transform(data1)
print(data1)
print(sd)

a=np.max(data1)
print(a)
a=a-data1
print(a)

data['length']=data['length']/data['length'].max()
data['width']=data['width']/data['width'].max()
data['height']=data['height']/data['height'].max()

data[['length', 'width','height']]

"""min-max scale it is func

zero will be replacd this should not be done

min and max num will not be solid zero


rescales the feature values to a range between 0 and 1,

> Indented block
 result=(value-min)/(max-min)



"""

from sklearn.preprocessing import MinMaxScaler
data1=[[1,2],[3,4],[5,6]]
scalar=MinMaxScaler()
sd=scalar.fit_transform(data1)
print(data1)
print(sd)