# -*- coding: utf-8 -*-
"""Iris_ANN_100%.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X1VLz5JA6GWncaFvgqaZcb1xAErIcAJ5
"""

import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target.reshape(-1, 1)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# One-hot encode the target variable
encoder = OneHotEncoder(sparse=False)
y_onehot = encoder.fit_transform(y)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)

# Standardize the input features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Neural Network architecture
input_size = X_train.shape[1]
hidden_size = 8
output_size = y_onehot.shape[1]

# Hyperparameters
learning_rate = 0.01
epochs = 5000

def initialize_weights(input_size, hidden_size, output_size):
    wh = np.random.uniform(size=(input_size, hidden_size))
    bh=np.random.uniform(size=(1,hidden_size))
    wo = np.random.uniform(size=(hidden_size, output_size))
    bo=np.random.uniform(size=(1,output_size))# Initializing biases for the output layer
    return wh, bh, wo, bo
# Initialize weights and biases
wh, bh, wo, bo = initialize_weights(input_size, hidden_size, output_size)

def forward_propagation(input_data, wh, bh, wo, bo):
    hidden_layer_input = np.dot(input_data, wh) + bh
    hidden_layer_output = sigmoid(hidden_layer_input)
    output_layer_input = np.dot(hidden_layer_output, wo) + bo
    final_output = sigmoid(output_layer_input)
    return hidden_layer_output, final_output

def backward_propagation(input_data, target_data, hidden_layer_output, final_output, wh, wo, bh, bo, learning_rate):
    output_error = final_output - target_data
    output_gradient = sigmoid_derivative(final_output)
    output_gradient *= output_error

    hidden_error = output_gradient.dot(wo.T)
    hidden_gradient = sigmoid_derivative(hidden_layer_output)
    hidden_gradient *= hidden_error

    wo -= learning_rate * hidden_layer_output.T.dot(output_gradient)
    bo -= learning_rate * np.sum(output_gradient, axis=0, keepdims=True)
    wh -= learning_rate * input_data.T.dot(hidden_gradient)
    bh -= learning_rate * np.sum(hidden_gradient, axis=0, keepdims=True)



# Train the neural network
for epoch in range(epochs):
    # Forward Propagation
    hidden_layer_output, final_output = forward_propagation(X_train, wh, bh, wo, bo)

    # Backward Propagation
    backward_propagation(X_train, y_train, hidden_layer_output, final_output, wh, wo, bh, bo, learning_rate)

    # Print the mean squared error for every 100 epochs
    if epoch % 100 == 0:
        mse = np.mean((final_output - y_train) ** 2)
        print(f'Epoch {epoch}, Mean Squared Error: {mse}')

# Make predictions on the test set
_, test_predictions = forward_propagation(X_test, wh, bh, wo, bo)

# Convert one-hot encoded predictions to class labels
predicted_labels = np.argmax(test_predictions, axis=1)

# Convert one-hot encoded true labels to class labels
true_labels = np.argmax(y_test, axis=1)

# Calculate accuracy on the test set
accuracy = accuracy_score(true_labels, predicted_labels)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

