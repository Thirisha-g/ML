# -*- coding: utf-8 -*-
"""RNN_TensorFlow Keras_SpamHam_Dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/173WvqWmLoi7t2fAAz4dZJx1PUGqhOBHD
"""

import pandas as pd

df=pd.read_csv('https://raw.githubusercontent.com/soaicbe/ml/master/spam.csv')

df

df.head()

df.isnull().sum()

df.describe()

df.info()

import numpy as np
from sklearn.model_selection import train_test_split

X=df['Message']
y = df['Category']

for i in range (len(y)):
  if y[i]=='spam':
    y[i]=0
  else:
    y[i]=1

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)

X_train=X_train.to_list()
y_train=y_train.to_list()
X_test=X_test.to_list()
y_test=y_test.to_list()

X_train

y_train

"""## 3. Building the Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras import Input

model = Sequential()
model.add(Input(shape=(1,), dtype="string"))

"""### 3.1 Text Vectorization"""

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

max_tokens = 20
max_len = 5
vectorize_layer = TextVectorization(
  # Max vocab size. Any words outside of the max_tokens most common ones
  # will be treated the same way: as "out of vocabulary" (OOV) tokens.
  max_tokens=max_tokens,
  # Output integer indices, one per string token
  output_mode="int",
  # Always pad or truncate to exactly this many tokens
  output_sequence_length=max_len,
)

# Call adapt(), which fits the TextVectorization layer to our text dataset.
# This is when the max_tokens most common words (i.e. the vocabulary) are selected.
vectorize_layer.adapt(X_train)

reverse_word_index = vectorize_layer.get_vocabulary()
reverse_word_index

vectorize_layer.get_weights()

model.add(vectorize_layer)

model.summary()

"""## 3.2 Embedding"""

from tensorflow.keras.layers import Embedding

# Note that we're using max_tokens + 1 here, since there's an
# out-of-vocabulary (OOV) token that gets added to the vocab.
model.add(Embedding(max_tokens + 1, 8))

"""## 3.3 The Recurrent Layer"""

from tensorflow.keras.layers import LSTM, SimpleRNN

# 64 is the "units" parameter, which is the
# dimensionality of the output space.
model.add(LSTM(16))
# model.add(SimpleRNN(16))

from tensorflow.keras.layers import Dense

model.add(Dense(16, activation="relu"))
model.add(Dense(1, activation="sigmoid"))

model.summary()

"""## 4. Compiling the Model

"""

model.compile(
  optimizer='adam',
  loss='binary_crossentropy',
  metrics=['accuracy'],
)

"""## 4. Compiling the Model

"""

history = model.fit(X_train, y_train, epochs=200)

import matplotlib.pylab as plt

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

print(model.predict([
    "Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.",
]))

print(model.predict([
 "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ãº1.20 POBOXox36504W45WQ 16+",
]))

# First get the weights of the embedding layer
e = model.layers[1]
weights = e.get_weights()[0]
print(weights.shape) # shape: (vocab_size, embedding_dim)